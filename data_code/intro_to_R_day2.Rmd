---
title: "Intro to R - Day 2"
author: "Daniel Fuller"
output:
  html_document:
    keep_md: true
  pdf_document: default
---

```{r}
library(tidyverse)
library(gtsummary)
library(vtable)
library(readxl)
library(reportRmd)
library(infer)
library(rstatix)
```

## Intro to R Day 2

### Importing data set in CSV file named `data1`

Make sure you check your file extension. If you have an excel file use `readxl` package or the `GUI` 

#### Tidyverse method

```{r}
data1 <- read_csv("dataset1.csv")

## Difference between Markdown and running code
#data1 <- read_csv("data_code/data1.csv") 
```

#### Base R method

```{r}
data1_base <- read.csv("dataset1.csv", header = TRUE)
```

### to see first 10 observations

```{r}
head(data1,10)
```

### Importing data set in CSV file named ‘data2’

```{r}
data2 <- read_csv("dataset1.csv")
```

### Merging data1 and data2

First we want to create two artificial datasets to merge. We are just going to select a few variables from each then put the data back together.

```{r}
data1 <- dplyr::select(data1, id, sex, ethgrp, weight, age, cvd)
data2 <- dplyr::select(data2, id, stroke, smoking, Cancer, ldl1, ldl2)
```

### Joining data 

In computer science the term is join. Some stat software uses the term merge. 

```{r}
data_merge <- dplyr::full_join(data1, data2) 

data_merge1 <- dplyr::full_join(data1, data2, by = join_by(id))

data_merge2 <- dplyr::full_join(data1, data2,by = join_by(id == id))

vt(data_merge)
```

### Importing test data

```{r}
test <- read_csv("test.csv")
head(test,10)
glimpse(test)
```

### Create categorial variable ‘agecat’ using age

Here we are introducing three new things `%>%` (`pipe operator`), `mutate`, and `case_when`

* `%>%` or `|>` (`pipe operator`) = Signifies to run the analysis and move down to the next function. 
* `mutate` = The verb for create a new variable from an old variable
* `case_when`= An `if_else` type function 

```{r}
### Tidyverse Method
test %>% summary(age)

### Base R Method
summary(test$age)
```

#### Recoding Age

```{r}
test <- test %>% 
          mutate(age_cat = case_when(
            age < 45 ~ "<45",
            age >= 45 & age < 50 ~ "45-49",
            age >= 50 & age < 59 ~ "50-59",
            age >= 60 & age < 65 ~ "60-64",  
            TRUE ~ "65+"
          ))

table(test$age, test$age_cat)
```

```{r}
### Tidyverse Method
test %>% count(age_cat)

### Base R Method
count(test, age_cat)
```

### Writing a test to see if that worked

```{r}
### Base R Method
table(test$age, test$age_cat)
```

### Check frequency distribution of a categorical variable

#### Check frequency distribution of gender

```{r}
table(test$gender)
```

#### Cross tabulation of gender and stroke

```{r}
table(test$gender, test$stroke)
```

#### Recoding other variables

#### Recoding Ethnicity

```{r}
table(test$ethgrp)

test <- test %>% 
          mutate(ethnicity = case_when(
            ethgrp == 1 ~ "Black",
            ethgrp == 2 ~ "White",
            ethgrp == 3 ~ "Other"
          ))

table(test$ethgrp, test$ethnicity)
```

## Create a table 1

### vtable package

```{r}
st(test) 
```

### reportRmd package

```{r}
rm_covsum(data = test, 
          covs = c('age', 'gender', 'ethnicity', 'stroke', 'Cancer', 'cvd'),
          show.tests=TRUE)        
```

### Chi-square test 

Lots of the previous existing packages work with traditional coding rather than Tidyverse style. Here we are going to work on some of those older methods. If you want to use the newer versions use the [infer](https://infer.tidymodels.org/index.html) package. 

> A chi-squared test (also chi-square or χ2 test) is a statistical hypothesis test used in the analysis of contingency tables when the sample sizes are large. In simpler terms, this test is primarily used to examine whether two categorical variables (two dimensions of the contingency table) are independent in influencing the test statistic. [Wiki](https://en.wikipedia.org/wiki/Chi-squared_test)

```{r}
### Tidyverse
test$stroke_factor <- as.factor(test$stroke)

table(test$gender, test$stroke)

infer::chisq_test(test, gender ~ stroke_factor)  ## Not on conflicting packages

### Base R
chisq.test(test$gender, test$stroke)
```

### Fishers exact test

> Fisher's exact test is a statistical significance test used in the analysis of contingency tables. Although in practice it is employed when sample sizes are small, it is valid for all sample sizes. [Wiki](https://en.wikipedia.org/wiki/Fisher%27s_exact_test)

```{r}
xtab <- table(test$gender, test$stroke)

### Tidyverse
fisher_test(xtab, detailed = TRUE)

### Base R
fisher.test(test$gender, test$stroke)
```

## Shapiro Wilk Test

> The null-hypothesis of this test is that the population is normally distributed. If the p value is less than the chosen alpha level, then the null hypothesis is rejected and there is evidence that the data tested are not normally distributed.[Wiki](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test)

#### First, we check if data follow Normal distribution
#### Normality test (if p>0.05: data are normally distributed)
####  Want to compare mean ages between male and female

####  Check histogtam
```{r}
hist_age_gender <- ggplot(test, aes(age)) +
  geom_histogram() + 
  facet_wrap(~ gender)

plot(hist_age_gender)
```

### Shapiro test females

```{r}
test_data_female <- filter(test, gender == "f")
shapiro.test(test_data_female$age)
```

#### Shapiro test males

```{r}
test_data_male <- filter(test, gender == "m")
shapiro.test(test_data_male$age)
```

## t-test

> Student's t-test is a statistical test used to test whether the difference between the response of two groups is statistically significant or not. It is any statistical hypothesis test in which the test statistic follows a Student's t-distribution under the null hypothesis. [Wiki](https://en.wikipedia.org/wiki/Student%27s_t-test)

Lots of things we can do with t-tests. Generic formalution for many tests is `test_function`(`variable1` ~ `variable1`, data = `my_data`)

```{r}
help(t.test)
t.test(age ~ gender, data = test, var.equal = FALSE)
```

### Check variance

```{r}
var.test(age ~ gender, data = test)
```

### Variances are equal based on the test

```{r}
t.test(age ~ gender, data = test, var.equal = TRUE)
```

## Wilcoxon signed-rank test

### Non-parametric test if data are not normally distributed

> The Wilcoxon signed-rank test is a non-parametric rank test for statistical hypothesis testing used either to test the location of a population based on a sample of data, or to compare the locations of two populations using two matched samples. The one-sample version serves a purpose similar to that of the one-sample Student's t-test. For two matched samples, it is a paired difference test like the paired Student's t-test (also known as the "t-test for matched pairs" or "t-test for dependent samples"). [Wiki](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test)

```{r}
wilcox.test(age ~ gender, data = test)
```

### Paired t-test for two dependent samples

```{r}
t.test(test$ldl1, test$ldl2, paired = TRUE, var.equal = FALSE)
```

## Pearson correlation coefficient

> In statistics, the Pearson correlation coefficient is a correlation coefficient that measures linear correlation between two sets of data. It is the ratio between the covariance of two variables and the product of their standard deviations; thus, it is essentially a normalized measurement of the covariance, such that the result always has a value between −1 and 1. [Wiki](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)

```{r}
cor.test(test$ldl1, test$ldl2,  method = "pearson") 
```

## Spearman correlation coefficient

> In statistics, Spearman's rank correlation coefficient or Spearman's ρ is a number ranging from -1 to 1 that indicates how strongly two sets of ranks are correlated. It could be used in a situation where one only has ranked data, such as a tally of gold, silver, and bronze medals. [Wiki](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)

```{r}
cor.test(test$ldl1, test$ldl2,  method = "spearman") 
```

## Linear regression

Conduct linear regression model between dependent and independent variables. Age is continuous, gender is categorical, ldl is continuous variable.

> In statistics, linear regression is a model that estimates the relationship between a scalar response (dependent variable) and one or more explanatory variables (regressor or independent variable). A model with exactly one explanatory variable is a simple linear regression; a model with two or more explanatory variables is a multiple linear regression. This term is distinct from multivariate linear regression, which predicts multiple correlated dependent variables rather than a single dependent variable. [Wiki](https://en.wikipedia.org/wiki/Linear_regression#Estimation_methods)

```{r}
linear_model <- lm(age ~ as.factor(gender) + ldl1, data = test)
summary(linear_model)

gtsummary::tbl_regression(linear_model)
```

## Logistic regression

> In statistics, a logistic model (or logit model) is a statistical model that models the log-odds of an event as a linear combination of one or more independent variables. In regression analysis, logistic regression (or logit regression) estimates the parameters of a logistic model (the coefficients in the linear or non linear combinations). [Wiki](https://en.wikipedia.org/wiki/Logistic_regression)

```{r}
logistic_model <- glm(Cancer ~ as.factor(gender) + ldl1 + smoking, data = test, family = "binomial")
summary(logistic_model)
```

### Get ORs

#### Old School Way

```{r}
exp(cbind(OR = coef(logistic_model), confint(logistic_model)))
```

#### New School Way
```{r}
tbl_regression(logistic_model, exponentiate = TRUE) 
```

### One-way ANOVA

> In statistics, one-way analysis of variance (or one-way ANOVA) is a technique to compare whether two or more samples' means are significantly different (using the F distribution). This analysis of variance technique requires a numeric response variable "Y" and a single explanatory variable "X", hence "one-way". [Wiki](https://en.wikipedia.org/wiki/One-way_analysis_of_variance)

Pass arguments to aov() function for an ANOVA test

```{r}
one_anova <- aov(age ~ ethgrp, data = test)
summary(one_anova)
```

### Non-parametric ANOVA (Kruskal–Wallis test)

> The Kruskal–Wallis test by ranks, Kruskal–Wallis H {\displaystyle H} test (named after William Kruskal and W. Allen Wallis), or one-way ANOVA on ranks is a non-parametric statistical test for testing whether samples originate from the same distribution. It is used for comparing two or more independent samples of equal or different sample sizes. It extends the Mann–Whitney U test, which is used for comparing only two groups. [Wiki](https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_test)

```{r}
kruskal.test(age ~ ethgrp, data = test)
```